{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOWTk9UgytX5",
        "outputId": "88e591e5-89be-4879-86c2-a90e5b64be37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "RDO-PTQ Demo: Rate-Distortion Optimized Quantization\n",
            "======================================================================\n",
            "\n",
            "[Step 1] Creating pre-trained model...\n",
            "Total parameters: 370,816\n",
            "FP32 model size: 1.41 MB\n",
            "\n",
            "[Step 2] Preparing calibration data...\n",
            "Calibration set: 20 images\n",
            "\n",
            "[Step 3] Initializing RDO-PTQ quantizer...\n",
            "Target: INT8, Lambda: 0.005\n",
            "\n",
            "[Step 4] Quantizing model...\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Found 3 quantizable layers\n",
            "Quantizing to 8-bit with lambda=0.005\n",
            "\n",
            "[1/3] Processing: encoder.0\n",
            "Quantizing layer: encoder.0\n",
            "  Iter   0/150, Loss: 0.000002, LR: 5.00e-04\n",
            "  Iter  50/150, Loss: 0.000024, LR: 3.70e-04\n",
            "  Iter 100/150, Loss: 0.000023, LR: 1.20e-04\n",
            "  Iter 149/150, Loss: 0.000023, LR: 0.00e+00\n",
            "\n",
            "[2/3] Processing: encoder.2\n",
            "Quantizing layer: encoder.2\n",
            "  Iter   0/150, Loss: 0.000002, LR: 5.00e-04\n",
            "  Iter  50/150, Loss: 0.000013, LR: 3.70e-04\n",
            "  Iter 100/150, Loss: 0.000012, LR: 1.20e-04\n",
            "  Iter 149/150, Loss: 0.000012, LR: 0.00e+00\n",
            "\n",
            "[3/3] Processing: encoder.4\n",
            "Quantizing layer: encoder.4\n",
            "  Iter   0/150, Loss: 0.000003, LR: 5.00e-04\n",
            "  Iter  50/150, Loss: 0.000012, LR: 3.70e-04\n",
            "  Iter 100/150, Loss: 0.000010, LR: 1.20e-04\n",
            "  Iter 149/150, Loss: 0.000010, LR: 0.00e+00\n",
            "\n",
            "Quantization complete!\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "[Step 5] Evaluating quantized model...\n",
            "\n",
            " Compression Results:\n",
            "  FP32 size: 1.41 MB\n",
            "  INT8 size: 0.35 MB\n",
            "  Compression: 4.0x\n",
            "\n",
            " Quality Metrics (on test images):\n",
            "  MSE: 0.000152\n",
            "  PSNR: 19.97 dB\n",
            "  Max difference: 0.0944\n",
            "  Relative error: 67.47%\n",
            "\n",
            " Exporting quantized parameters...\n",
            "Exported quantized parameters to quantized_model.pth\n",
            "\n",
            "======================================================================\n",
            "Demo complete!\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2817481793.py:375: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  psnr = 10 * torch.log10(torch.tensor(original_out.max()**2 / (mse + 1e-10))).item()\n"
          ]
        }
      ],
      "source": [
        "# BasicRDOPTQ\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from typing import List, Tuple, Optional, Dict\n",
        "import copy\n",
        "import numpy as np\n",
        "\n",
        "class QuantizedConv2d(nn.Module):\n",
        "    def __init__(self, conv_layer: nn.Conv2d, bit_width: int = 8):\n",
        "        super().__init__()\n",
        "        self.in_channels = conv_layer.in_channels\n",
        "        self.out_channels = conv_layer.out_channels\n",
        "        self.kernel_size = conv_layer.kernel_size\n",
        "        self.stride = conv_layer.stride\n",
        "        self.padding = conv_layer.padding\n",
        "        self.bit_width = bit_width\n",
        "        self.n_levels = 2 ** bit_width - 1\n",
        "\n",
        "\n",
        "        self.register_buffer('weight_float', conv_layer.weight.data.clone())\n",
        "        if conv_layer.bias is not None:\n",
        "            self.register_buffer('bias_float', conv_layer.bias.data.clone())\n",
        "        else:\n",
        "            self.bias_float = None\n",
        "\n",
        "\n",
        "        self.weight_scale = nn.Parameter(torch.ones(self.out_channels, 1, 1, 1))\n",
        "        self.weight_zero_point = nn.Parameter(torch.zeros(self.out_channels, 1, 1, 1))\n",
        "\n",
        "        if self.bias_float is not None:\n",
        "            self.bias_scale = nn.Parameter(torch.ones(self.out_channels))\n",
        "            self.bias_zero_point = nn.Parameter(torch.zeros(self.out_channels))\n",
        "\n",
        "\n",
        "        self._initialize_scales()\n",
        "\n",
        "    def _initialize_scales(self):\n",
        "        with torch.no_grad():\n",
        "            for c in range(self.out_channels):\n",
        "                w_channel = self.weight_float[c]\n",
        "                w_min = w_channel.min()\n",
        "                w_max = w_channel.max()\n",
        "                scale = (w_max - w_min) / self.n_levels\n",
        "                zero_point = -w_min / (scale + 1e-8)\n",
        "                self.weight_scale.data[c] = scale\n",
        "                self.weight_zero_point.data[c] = zero_point.clamp(0, self.n_levels)\n",
        "\n",
        "            if self.bias_float is not None:\n",
        "                for c in range(self.out_channels):\n",
        "                    b_val = self.bias_float[c]\n",
        "                    b_abs = abs(b_val) + 1e-8\n",
        "                    self.bias_scale.data[c] = b_abs / (self.n_levels / 2)\n",
        "                    self.bias_zero_point.data[c] = self.n_levels / 2\n",
        "\n",
        "    def quantize_weight(self):\n",
        "        w_normalized = self.weight_float / (self.weight_scale + 1e-8) + self.weight_zero_point\n",
        "        w_quant = torch.clamp(torch.round(w_normalized), 0, self.n_levels)\n",
        "        w_dequant = (w_quant - self.weight_zero_point) * self.weight_scale\n",
        "        return w_dequant\n",
        "\n",
        "    def quantize_bias(self):\n",
        "        if self.bias_float is None:\n",
        "            return None\n",
        "        b_normalized = self.bias_float / (self.bias_scale + 1e-8) + self.bias_zero_point\n",
        "        b_quant = torch.clamp(torch.round(b_normalized), 0, self.n_levels)\n",
        "        b_dequant = (b_quant - self.bias_zero_point) * self.bias_scale\n",
        "        return b_dequant\n",
        "\n",
        "    def forward(self, x):\n",
        "        w_quant = self.quantize_weight()\n",
        "        b_quant = self.quantize_bias()\n",
        "\n",
        "        return nn.functional.conv2d(\n",
        "            x, w_quant, b_quant,\n",
        "            stride=self.stride,\n",
        "            padding=self.padding\n",
        "        )\n",
        "\n",
        "    def get_quantized_params(self):\n",
        "        w_normalized = self.weight_float / (self.weight_scale + 1e-8) + self.weight_zero_point\n",
        "        w_quant_int = torch.clamp(torch.round(w_normalized), 0, self.n_levels).to(torch.int8)\n",
        "\n",
        "        if self.bias_float is not None:\n",
        "            b_normalized = self.bias_float / (self.bias_scale + 1e-8) + self.bias_zero_point\n",
        "            b_quant_int = torch.clamp(torch.round(b_normalized), 0, self.n_levels).to(torch.int8)\n",
        "        else:\n",
        "            b_quant_int = None\n",
        "\n",
        "        return {\n",
        "            'weight_int8': w_quant_int,\n",
        "            'weight_scale': self.weight_scale,\n",
        "            'weight_zero_point': self.weight_zero_point,\n",
        "            'bias_int8': b_quant_int,\n",
        "            'bias_scale': self.bias_scale if self.bias_float is not None else None,\n",
        "            'bias_zero_point': self.bias_zero_point if self.bias_float is not None else None\n",
        "        }\n",
        "\n",
        "\n",
        "class RDO_PTQ:\n",
        "    def __init__(self,\n",
        "                 model: nn.Module,\n",
        "                 bit_width: int = 8,\n",
        "                 lambda_rd: float = 0.01,\n",
        "                 device: str = 'cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "        self.model = model.to(device)\n",
        "        self.bit_width = bit_width\n",
        "        self.lambda_rd = lambda_rd\n",
        "        self.device = device\n",
        "        self.quantized_layers = {}\n",
        "\n",
        "    def _get_quantizable_layers(self) -> List[Tuple[str, nn.Module]]:\n",
        "        quantizable = []\n",
        "        for name, module in self.model.named_modules():\n",
        "            if isinstance(module, (nn.Conv2d, nn.ConvTranspose2d)):\n",
        "                quantizable.append((name, module))\n",
        "        return quantizable\n",
        "\n",
        "    def _compute_rate_distortion_loss(self,\n",
        "                                     original_output: torch.Tensor,\n",
        "                                     quantized_output: torch.Tensor,\n",
        "                                     quantized_layer: QuantizedConv2d) -> torch.Tensor:\n",
        "\n",
        "\n",
        "        distortion = torch.mean((original_output - quantized_output) ** 2)\n",
        "\n",
        "\n",
        "        w_quant = quantized_layer.quantize_weight()\n",
        "        num_params = w_quant.numel()\n",
        "        if quantized_layer.bias_float is not None:\n",
        "            num_params += quantized_layer.bias_float.numel()\n",
        "\n",
        "\n",
        "        rate_penalty = torch.mean(torch.abs(quantized_layer.weight_scale))\n",
        "\n",
        "\n",
        "        rd_loss = distortion + self.lambda_rd * rate_penalty\n",
        "\n",
        "        return rd_loss\n",
        "\n",
        "    def _get_layer_activations(self, layer_name: str,\n",
        "                               inputs: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "\n",
        "        activations = {}\n",
        "\n",
        "        def hook_fn(module, input, output):\n",
        "            activations['input'] = input[0].detach().clone()\n",
        "            activations['output'] = output.detach().clone()\n",
        "\n",
        "\n",
        "        target_layer = dict(self.model.named_modules())[layer_name]\n",
        "        handle = target_layer.register_forward_hook(hook_fn)\n",
        "\n",
        "\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            _ = self.model(inputs)\n",
        "\n",
        "\n",
        "        handle.remove()\n",
        "\n",
        "        return activations['input'], activations['output']\n",
        "\n",
        "    def quantize_layer(self,\n",
        "                      layer_name: str,\n",
        "                      layer: nn.Conv2d,\n",
        "                      calibration_inputs: torch.Tensor,\n",
        "                      original_outputs: torch.Tensor,\n",
        "                      num_iterations: int = 200,\n",
        "                      lr: float = 1e-4) -> QuantizedConv2d:\n",
        "\n",
        "        print(f\"Quantizing layer: {layer_name}\")\n",
        "\n",
        "\n",
        "        q_layer = QuantizedConv2d(layer, self.bit_width).to(self.device)\n",
        "\n",
        "\n",
        "        params = [q_layer.weight_scale, q_layer.weight_zero_point]\n",
        "        if q_layer.bias_float is not None:\n",
        "            params.extend([q_layer.bias_scale, q_layer.bias_zero_point])\n",
        "\n",
        "        optimizer = optim.Adam(params, lr=lr)\n",
        "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, num_iterations)\n",
        "\n",
        "        best_loss = float('inf')\n",
        "        best_state = None\n",
        "\n",
        "\n",
        "        q_layer.train()\n",
        "        for iteration in range(num_iterations):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "\n",
        "            quantized_output = q_layer(calibration_inputs)\n",
        "\n",
        "\n",
        "            loss = self._compute_rate_distortion_loss(\n",
        "                original_outputs, quantized_output, q_layer\n",
        "            )\n",
        "\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(params, max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "\n",
        "            if loss.item() < best_loss:\n",
        "                best_loss = loss.item()\n",
        "                best_state = {\n",
        "                    'weight_scale': q_layer.weight_scale.data.clone(),\n",
        "                    'weight_zero_point': q_layer.weight_zero_point.data.clone(),\n",
        "                }\n",
        "                if q_layer.bias_float is not None:\n",
        "                    best_state['bias_scale'] = q_layer.bias_scale.data.clone()\n",
        "                    best_state['bias_zero_point'] = q_layer.bias_zero_point.data.clone()\n",
        "\n",
        "            if iteration % 50 == 0 or iteration == num_iterations - 1:\n",
        "                print(f\"  Iter {iteration:3d}/{num_iterations}, Loss: {loss.item():.6f}, LR: {scheduler.get_last_lr()[0]:.2e}\")\n",
        "\n",
        "\n",
        "        if best_state is not None:\n",
        "            q_layer.weight_scale.data = best_state['weight_scale']\n",
        "            q_layer.weight_zero_point.data = best_state['weight_zero_point']\n",
        "            if q_layer.bias_float is not None:\n",
        "                q_layer.bias_scale.data = best_state['bias_scale']\n",
        "                q_layer.bias_zero_point.data = best_state['bias_zero_point']\n",
        "\n",
        "        q_layer.eval()\n",
        "        return q_layer\n",
        "\n",
        "    def quantize_model(self,\n",
        "                      calibration_images: torch.Tensor,\n",
        "                      num_iterations: int = 200,\n",
        "                      lr: float = 1e-4) -> nn.Module:\n",
        "\n",
        "        calibration_images = calibration_images.to(self.device)\n",
        "        quantizable_layers = self._get_quantizable_layers()\n",
        "\n",
        "        print(f\"\\nFound {len(quantizable_layers)} quantizable layers\")\n",
        "        print(f\"Quantizing to {self.bit_width}-bit with lambda={self.lambda_rd}\\n\")\n",
        "\n",
        "\n",
        "        quantized_model = copy.deepcopy(self.model)\n",
        "\n",
        "\n",
        "        for idx, (layer_name, layer) in enumerate(quantizable_layers):\n",
        "            print(f\"[{idx+1}/{len(quantizable_layers)}] Processing: {layer_name}\")\n",
        "\n",
        "\n",
        "            layer_inputs, layer_outputs = self._get_layer_activations(\n",
        "                layer_name, calibration_images\n",
        "            )\n",
        "\n",
        "\n",
        "            q_layer = self.quantize_layer(\n",
        "                layer_name, layer, layer_inputs, layer_outputs,\n",
        "                num_iterations, lr\n",
        "            )\n",
        "\n",
        "\n",
        "            self._replace_layer(quantized_model, layer_name, q_layer)\n",
        "            self.quantized_layers[layer_name] = q_layer\n",
        "\n",
        "            print()\n",
        "\n",
        "        print(\"Quantization complete!\\n\")\n",
        "        return quantized_model\n",
        "\n",
        "    def _replace_layer(self, model: nn.Module, layer_name: str, new_layer: nn.Module):\n",
        "\n",
        "        parts = layer_name.split('.')\n",
        "        parent = model\n",
        "        for part in parts[:-1]:\n",
        "            parent = getattr(parent, part)\n",
        "        setattr(parent, parts[-1], new_layer)\n",
        "\n",
        "    def export_quantized_params(self, output_path: str = 'quantized_params.pth'):\n",
        "\n",
        "        quantized_params = {}\n",
        "        for name, q_layer in self.quantized_layers.items():\n",
        "            quantized_params[name] = q_layer.get_quantized_params()\n",
        "\n",
        "        torch.save(quantized_params, output_path)\n",
        "        print(f\"Exported quantized parameters to {output_path}\")\n",
        "        return quantized_params\n",
        "\n",
        "\n",
        "def demo_rdo_ptq():\n",
        "\n",
        "    print(\"=\"*70)\n",
        "    print(\"RDO-PTQ Demo: Rate-Distortion Optimized Quantization\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "\n",
        "    class SimpleImageEncoder(nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "            self.encoder = nn.Sequential(\n",
        "                nn.Conv2d(3, 64, 3, padding=1),\n",
        "                nn.ReLU(),\n",
        "                nn.Conv2d(64, 128, 3, stride=2, padding=1),\n",
        "                nn.ReLU(),\n",
        "                nn.Conv2d(128, 256, 3, stride=2, padding=1),\n",
        "                nn.ReLU()\n",
        "            )\n",
        "\n",
        "        def forward(self, x):\n",
        "            return self.encoder(x)\n",
        "\n",
        "\n",
        "    print(\"\\n[Step 1] Creating pre-trained model...\")\n",
        "    model = SimpleImageEncoder()\n",
        "    for param in model.parameters():\n",
        "        nn.init.normal_(param, mean=0, std=0.02)\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"Total parameters: {total_params:,}\")\n",
        "    print(f\"FP32 model size: {total_params * 4 / (1024**2):.2f} MB\")\n",
        "\n",
        "\n",
        "    print(\"\\n[Step 2] Preparing calibration data...\")\n",
        "    num_calibration_images = 20\n",
        "    calibration_data = torch.randn(num_calibration_images, 3, 128, 128)\n",
        "    print(f\"Calibration set: {num_calibration_images} images\")\n",
        "\n",
        "\n",
        "    print(\"\\n[Step 3] Initializing RDO-PTQ quantizer...\")\n",
        "    bit_width = 8\n",
        "    lambda_rd = 0.005\n",
        "\n",
        "    rdo_ptq = RDO_PTQ(\n",
        "        model=model,\n",
        "        bit_width=bit_width,\n",
        "        lambda_rd=lambda_rd,\n",
        "        device='cpu'\n",
        "    )\n",
        "    print(f\"Target: INT{bit_width}, Lambda: {lambda_rd}\")\n",
        "\n",
        "\n",
        "    print(\"\\n[Step 4] Quantizing model...\")\n",
        "    print(\"-\" * 70)\n",
        "    quantized_model = rdo_ptq.quantize_model(\n",
        "        calibration_images=calibration_data,\n",
        "        num_iterations=150,\n",
        "        lr=5e-4\n",
        "    )\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "\n",
        "    print(\"[Step 5] Evaluating quantized model...\")\n",
        "\n",
        "    int8_size = total_params * (bit_width / 8) / (1024**2)\n",
        "    compression_ratio = (total_params * 4) / (total_params * bit_width / 8)\n",
        "\n",
        "    print(f\"\\n Compression Results:\")\n",
        "    print(f\"  FP32 size: {total_params * 4 / (1024**2):.2f} MB\")\n",
        "    print(f\"  INT8 size: {int8_size:.2f} MB\")\n",
        "    print(f\"  Compression: {compression_ratio:.1f}x\")\n",
        "\n",
        "\n",
        "    print(f\"\\n Quality Metrics (on test images):\")\n",
        "    test_images = torch.randn(10, 3, 128, 128)\n",
        "\n",
        "    model.eval()\n",
        "    quantized_model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        original_out = model(test_images)\n",
        "        quantized_out = quantized_model(test_images)\n",
        "\n",
        "        mse = torch.mean((original_out - quantized_out) ** 2).item()\n",
        "        psnr = 10 * torch.log10(torch.tensor(original_out.max()**2 / (mse + 1e-10))).item()\n",
        "        max_diff = torch.max(torch.abs(original_out - quantized_out)).item()\n",
        "        relative_error = (torch.mean(torch.abs(original_out - quantized_out)) /\n",
        "                         (torch.mean(torch.abs(original_out)) + 1e-10)).item() * 100\n",
        "\n",
        "    print(f\"  MSE: {mse:.6f}\")\n",
        "    print(f\"  PSNR: {psnr:.2f} dB\")\n",
        "    print(f\"  Max difference: {max_diff:.4f}\")\n",
        "    print(f\"  Relative error: {relative_error:.2f}%\")\n",
        "\n",
        "\n",
        "    print(f\"\\n Exporting quantized parameters...\")\n",
        "    rdo_ptq.export_quantized_params('quantized_model.pth')\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"Demo complete!\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    return model, quantized_model\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    original_model, quantized_model = demo_rdo_ptq()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ProductionRDOPTQ\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from typing import List, Tuple, Optional, Dict, Union\n",
        "import copy\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class QuantizedConv2d(nn.Module):\n",
        "\n",
        "    def __init__(self, conv_layer: nn.Conv2d, bit_width: int = 8, quantize_activations: bool = True):\n",
        "        super().__init__()\n",
        "        self.in_channels = conv_layer.in_channels\n",
        "        self.out_channels = conv_layer.out_channels\n",
        "        self.kernel_size = conv_layer.kernel_size\n",
        "        self.stride = conv_layer.stride\n",
        "        self.padding = conv_layer.padding\n",
        "        self.dilation = conv_layer.dilation\n",
        "        self.groups = conv_layer.groups\n",
        "        self.bit_width = bit_width\n",
        "        self._quantize_activations = quantize_activations\n",
        "\n",
        "\n",
        "        self.qmax = (2 ** (bit_width - 1)) - 1\n",
        "        self.qmin = -(2 ** (bit_width - 1))\n",
        "\n",
        "\n",
        "        self.register_buffer('weight_float', conv_layer.weight.data.clone())\n",
        "        if conv_layer.bias is not None:\n",
        "            self.register_buffer('bias_float', conv_layer.bias.data.clone())\n",
        "        else:\n",
        "            self.bias_float = None\n",
        "\n",
        "\n",
        "        self.weight_scale = nn.Parameter(torch.ones(self.out_channels, 1, 1, 1))\n",
        "\n",
        "        if self.bias_float is not None:\n",
        "            self.bias_scale = nn.Parameter(torch.ones(self.out_channels))\n",
        "\n",
        "\n",
        "        if self._quantize_activations:\n",
        "            self.act_scale = nn.Parameter(torch.ones(1))\n",
        "            self.register_buffer('act_running_min', torch.zeros(1))\n",
        "            self.register_buffer('act_running_max', torch.zeros(1))\n",
        "            self.register_buffer('num_batches_tracked', torch.tensor(0))\n",
        "\n",
        "        self._initialize_scales()\n",
        "\n",
        "    def _initialize_scales(self):\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            for c in range(self.out_channels):\n",
        "                w_channel = self.weight_float[c]\n",
        "                w_max_abs = torch.max(torch.abs(w_channel))\n",
        "                scale = w_max_abs / (self.qmax + 1e-8)\n",
        "                self.weight_scale.data[c] = scale.clamp(min=1e-8)\n",
        "\n",
        "\n",
        "            if self.bias_float is not None:\n",
        "                for c in range(self.out_channels):\n",
        "                    b_abs = torch.abs(self.bias_float[c])\n",
        "                    self.bias_scale.data[c] = (b_abs / (self.qmax + 1e-8)).clamp(min=1e-8)\n",
        "\n",
        "    def quantize_weights(self):\n",
        "\n",
        "        w_div = self.weight_float / self.weight_scale.clamp(min=1e-8)\n",
        "        w_quant = torch.clamp(torch.round(w_div), self.qmin, self.qmax)\n",
        "        w_dequant = w_quant * self.weight_scale\n",
        "        return w_dequant\n",
        "\n",
        "    def quantize_bias(self):\n",
        "\n",
        "        if self.bias_float is None:\n",
        "            return None\n",
        "        b_div = self.bias_float / self.bias_scale.clamp(min=1e-8)\n",
        "        b_quant = torch.clamp(torch.round(b_div), self.qmin, self.qmax)\n",
        "        b_dequant = b_quant * self.bias_scale\n",
        "        return b_dequant\n",
        "\n",
        "    def quantize_activations(self, x: torch.Tensor, training: bool = False):\n",
        "\n",
        "        if not self._quantize_activations:\n",
        "            return x\n",
        "\n",
        "        if training:\n",
        "\n",
        "            x_min = x.min()\n",
        "            x_max = x.max()\n",
        "\n",
        "            momentum = 0.1\n",
        "            self.act_running_min = (1 - momentum) * self.act_running_min + momentum * x_min\n",
        "            self.act_running_max = (1 - momentum) * self.act_running_max + momentum * x_max\n",
        "            self.num_batches_tracked += 1\n",
        "\n",
        "\n",
        "        x_max_abs = torch.max(torch.abs(self.act_running_min), torch.abs(self.act_running_max))\n",
        "        scale = x_max_abs / (self.qmax + 1e-8)\n",
        "        scale = scale.clamp(min=1e-8)\n",
        "\n",
        "\n",
        "        x_div = x / scale\n",
        "        x_quant = torch.clamp(torch.round(x_div), self.qmin, self.qmax)\n",
        "        x_dequant = x_quant * scale\n",
        "\n",
        "        return x_dequant\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        if self._quantize_activations:\n",
        "            x = self.quantize_activations(x, self.training)\n",
        "\n",
        "\n",
        "        w_quant = self.quantize_weights()\n",
        "        b_quant = self.quantize_bias()\n",
        "\n",
        "\n",
        "        out = nn.functional.conv2d(\n",
        "            x, w_quant, b_quant,\n",
        "            stride=self.stride,\n",
        "            padding=self.padding,\n",
        "            dilation=self.dilation,\n",
        "            groups=self.groups\n",
        "        )\n",
        "\n",
        "        return out\n",
        "\n",
        "    def compute_compression_rate(self) -> float:\n",
        "\n",
        "        with torch.no_grad():\n",
        "            w_div = self.weight_float / self.weight_scale.clamp(min=1e-8)\n",
        "            w_quant = torch.clamp(torch.round(w_div), self.qmin, self.qmax)\n",
        "\n",
        "\n",
        "            total_entropy = 0\n",
        "            for c in range(self.out_channels):\n",
        "                unique_vals = torch.unique(w_quant[c])\n",
        "                n_unique = len(unique_vals)\n",
        "                effective_bits = np.log2(n_unique + 1)\n",
        "                total_entropy += effective_bits * w_quant[c].numel()\n",
        "\n",
        "            total_weights = w_quant.numel()\n",
        "            avg_bits = total_entropy / total_weights\n",
        "\n",
        "        return avg_bits\n",
        "\n",
        "    def get_quantized_params(self) -> Dict[str, torch.Tensor]:\n",
        "\n",
        "        with torch.no_grad():\n",
        "            w_div = self.weight_float / self.weight_scale.clamp(min=1e-8)\n",
        "            w_int8 = torch.clamp(torch.round(w_div), self.qmin, self.qmax).to(torch.int8)\n",
        "\n",
        "            result = {\n",
        "                'weight_int8': w_int8,\n",
        "                'weight_scale': self.weight_scale.clone(),\n",
        "            }\n",
        "\n",
        "            if self.bias_float is not None:\n",
        "                b_div = self.bias_float / self.bias_scale.clamp(min=1e-8)\n",
        "                b_int8 = torch.clamp(torch.round(b_div), self.qmin, self.qmax).to(torch.int8)\n",
        "                result['bias_int8'] = b_int8\n",
        "                result['bias_scale'] = self.bias_scale.clone()\n",
        "\n",
        "            if self._quantize_activations:\n",
        "                result['act_scale'] = self.act_scale.clone()\n",
        "                result['act_min'] = self.act_running_min.clone()\n",
        "                result['act_max'] = self.act_running_max.clone()\n",
        "\n",
        "        return result\n",
        "\n",
        "\n",
        "class QuantizedLinear(nn.Module):\n",
        "\n",
        "    def __init__(self, linear_layer: nn.Linear, bit_width: int = 8, quantize_activations: bool = True):\n",
        "        super().__init__()\n",
        "        self.in_features = linear_layer.in_features\n",
        "        self.out_features = linear_layer.out_features\n",
        "        self.bit_width = bit_width\n",
        "        self._quantize_activations = quantize_activations\n",
        "\n",
        "        self.qmax = (2 ** (bit_width - 1)) - 1\n",
        "        self.qmin = -(2 ** (bit_width - 1))\n",
        "\n",
        "        self.register_buffer('weight_float', linear_layer.weight.data.clone())\n",
        "        if linear_layer.bias is not None:\n",
        "            self.register_buffer('bias_float', linear_layer.bias.data.clone())\n",
        "        else:\n",
        "            self.bias_float = None\n",
        "\n",
        "        self.weight_scale = nn.Parameter(torch.ones(self.out_features, 1))\n",
        "\n",
        "        if self.bias_float is not None:\n",
        "            self.bias_scale = nn.Parameter(torch.ones(self.out_features))\n",
        "\n",
        "        if self._quantize_activations:\n",
        "            self.act_scale = nn.Parameter(torch.ones(1))\n",
        "            self.register_buffer('act_running_min', torch.zeros(1))\n",
        "            self.register_buffer('act_running_max', torch.zeros(1))\n",
        "\n",
        "        self._initialize_scales()\n",
        "\n",
        "    def _initialize_scales(self):\n",
        "        with torch.no_grad():\n",
        "            for c in range(self.out_features):\n",
        "                w_row = self.weight_float[c]\n",
        "                w_max_abs = torch.max(torch.abs(w_row))\n",
        "                self.weight_scale.data[c] = (w_max_abs / (self.qmax + 1e-8)).clamp(min=1e-8)\n",
        "\n",
        "            if self.bias_float is not None:\n",
        "                for c in range(self.out_features):\n",
        "                    b_abs = torch.abs(self.bias_float[c])\n",
        "                    self.bias_scale.data[c] = (b_abs / (self.qmax + 1e-8)).clamp(min=1e-8)\n",
        "\n",
        "    def quantize_weights(self):\n",
        "        w_div = self.weight_float / self.weight_scale.clamp(min=1e-8)\n",
        "        w_quant = torch.clamp(torch.round(w_div), self.qmin, self.qmax)\n",
        "        return w_quant * self.weight_scale\n",
        "\n",
        "    def quantize_bias(self):\n",
        "        if self.bias_float is None:\n",
        "            return None\n",
        "        b_div = self.bias_float / self.bias_scale.clamp(min=1e-8)\n",
        "        b_quant = torch.clamp(torch.round(b_div), self.qmin, self.qmax)\n",
        "        return b_quant * self.bias_scale\n",
        "\n",
        "    def forward(self, x):\n",
        "        w_quant = self.quantize_weights()\n",
        "        b_quant = self.quantize_bias()\n",
        "        return nn.functional.linear(x, w_quant, b_quant)\n",
        "\n",
        "    def get_quantized_params(self):\n",
        "        with torch.no_grad():\n",
        "            w_div = self.weight_float / self.weight_scale.clamp(min=1e-8)\n",
        "            w_int8 = torch.clamp(torch.round(w_div), self.qmin, self.qmax).to(torch.int8)\n",
        "\n",
        "            result = {'weight_int8': w_int8, 'weight_scale': self.weight_scale.clone()}\n",
        "\n",
        "            if self.bias_float is not None:\n",
        "                b_div = self.bias_float / self.bias_scale.clamp(min=1e-8)\n",
        "                b_int8 = torch.clamp(torch.round(b_div), self.qmin, self.qmax).to(torch.int8)\n",
        "                result['bias_int8'] = b_int8\n",
        "                result['bias_scale'] = self.bias_scale.clone()\n",
        "\n",
        "        return result\n",
        "\n",
        "\n",
        "class ProductionRDO_PTQ:\n",
        "\n",
        "    def __init__(self,\n",
        "                 model: nn.Module,\n",
        "                 bit_width: int = 8,\n",
        "                 lambda_rd: float = 0.01,\n",
        "                 quantize_activations: bool = True,\n",
        "                 use_blocks: bool = True,\n",
        "                 mixed_precision: bool = False,\n",
        "                 device: str = 'cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "\n",
        "        self.model = model.to(device)\n",
        "        self.default_bit_width = bit_width\n",
        "        self.lambda_rd = lambda_rd\n",
        "        self.quantize_activations = quantize_activations\n",
        "        self.use_blocks = use_blocks\n",
        "        self.mixed_precision = mixed_precision\n",
        "        self.device = device\n",
        "\n",
        "        self.quantized_layers = {}\n",
        "        self.layer_bit_widths = {}\n",
        "        self.layer_sensitivities = {}\n",
        "\n",
        "    def _get_quantizable_layers(self) -> List[Tuple[str, nn.Module]]:\n",
        "\n",
        "        quantizable = []\n",
        "        for name, module in self.model.named_modules():\n",
        "            if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
        "                quantizable.append((name, module))\n",
        "        return quantizable\n",
        "\n",
        "    def _get_layer_blocks(self, layers: List[Tuple[str, nn.Module]], block_size: int = 3) -> List[List[Tuple[str, nn.Module]]]:\n",
        "\n",
        "        if not self.use_blocks:\n",
        "            return [[layer] for layer in layers]\n",
        "\n",
        "        blocks = []\n",
        "        for i in range(0, len(layers), block_size):\n",
        "            blocks.append(layers[i:i + block_size])\n",
        "        return blocks\n",
        "\n",
        "    def analyze_layer_sensitivity(self,\n",
        "                                  calibration_data: torch.Tensor,\n",
        "                                  num_samples: int = 100) -> Dict[str, float]:\n",
        "\n",
        "        logger.info(\"Analyzing layer sensitivities...\")\n",
        "\n",
        "        quantizable_layers = self._get_quantizable_layers()\n",
        "        sensitivities = {}\n",
        "\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "\n",
        "            baseline_output = self.model(calibration_data[:num_samples])\n",
        "\n",
        "        for layer_name, layer in quantizable_layers:\n",
        "\n",
        "            if isinstance(layer, nn.Conv2d):\n",
        "                temp_q_layer = QuantizedConv2d(layer, bit_width=4, quantize_activations=False)\n",
        "            else:\n",
        "                temp_q_layer = QuantizedLinear(layer, bit_width=4, quantize_activations=False)\n",
        "\n",
        "\n",
        "            original_layer = self._get_layer_by_name(self.model, layer_name)\n",
        "            self._replace_layer(self.model, layer_name, temp_q_layer)\n",
        "\n",
        "\n",
        "            with torch.no_grad():\n",
        "                quantized_output = self.model(calibration_data[:num_samples])\n",
        "                sensitivity = torch.mean((baseline_output - quantized_output) ** 2).item()\n",
        "\n",
        "            sensitivities[layer_name] = sensitivity\n",
        "\n",
        "\n",
        "            self._replace_layer(self.model, layer_name, original_layer)\n",
        "\n",
        "            logger.info(f\"  {layer_name}: sensitivity = {sensitivity:.6f}\")\n",
        "\n",
        "        self.layer_sensitivities = sensitivities\n",
        "        return sensitivities\n",
        "\n",
        "    def assign_bit_widths(self, total_bit_budget: Optional[float] = None):\n",
        "\n",
        "        if not self.mixed_precision or not self.layer_sensitivities:\n",
        "\n",
        "            for name, _ in self._get_quantizable_layers():\n",
        "                self.layer_bit_widths[name] = self.default_bit_width\n",
        "            return\n",
        "\n",
        "        logger.info(\"Assigning mixed precision bit-widths...\")\n",
        "\n",
        "\n",
        "        sensitivities = np.array(list(self.layer_sensitivities.values()))\n",
        "        sens_min, sens_max = sensitivities.min(), sensitivities.max()\n",
        "        sens_norm = (sensitivities - sens_min) / (sens_max - sens_min + 1e-8)\n",
        "\n",
        "\n",
        "        bit_options = [4, 6, 8]\n",
        "        layer_names = list(self.layer_sensitivities.keys())\n",
        "\n",
        "        for i, (name, sens) in enumerate(zip(layer_names, sens_norm)):\n",
        "            if sens > 0.7:\n",
        "                bit_width = 8\n",
        "            elif sens > 0.3:\n",
        "                bit_width = 6\n",
        "            else:\n",
        "                bit_width = 4\n",
        "\n",
        "            self.layer_bit_widths[name] = bit_width\n",
        "            logger.info(f\"  {name}: {bit_width}-bit (sensitivity={sens:.3f})\")\n",
        "\n",
        "    def _compute_hessian_trace(self,\n",
        "                               layer_outputs: torch.Tensor,\n",
        "                               target_outputs: torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "        loss = torch.mean((layer_outputs - target_outputs) ** 2)\n",
        "        grads = torch.autograd.grad(loss, layer_outputs, create_graph=True)[0]\n",
        "        hessian_trace = torch.mean(grads ** 2)\n",
        "        return hessian_trace\n",
        "\n",
        "    def _compute_rd_loss(self,\n",
        "                        original_output: torch.Tensor,\n",
        "                        quantized_output: torch.Tensor,\n",
        "                        quantized_layer: Union[QuantizedConv2d, QuantizedLinear],\n",
        "                        use_hessian: bool = False) -> Tuple[torch.Tensor, Dict[str, float]]:\n",
        "\n",
        "        distortion = torch.mean((original_output - quantized_output) ** 2)\n",
        "\n",
        "\n",
        "        rate = torch.mean(torch.log(quantized_layer.weight_scale.clamp(min=1e-8) + 1e-8))\n",
        "\n",
        "\n",
        "        if use_hessian:\n",
        "            hessian_trace = self._compute_hessian_trace(quantized_output, original_output)\n",
        "            distortion = distortion * (1.0 + 0.1 * hessian_trace)\n",
        "\n",
        "\n",
        "        total_loss = distortion + self.lambda_rd * rate\n",
        "\n",
        "        metrics = {\n",
        "            'distortion': distortion.item(),\n",
        "            'rate': rate.item(),\n",
        "            'total': total_loss.item()\n",
        "        }\n",
        "\n",
        "        return total_loss, metrics\n",
        "\n",
        "    def _get_layer_activations(self,\n",
        "                               layer_name: str,\n",
        "                               inputs: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "\n",
        "        activations = {}\n",
        "\n",
        "        def hook_fn(module, inp, output):\n",
        "            activations['input'] = inp[0].detach().clone()\n",
        "            activations['output'] = output.detach().clone()\n",
        "\n",
        "        target_layer = self._get_layer_by_name(self.model, layer_name)\n",
        "        handle = target_layer.register_forward_hook(hook_fn)\n",
        "\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            _ = self.model(inputs)\n",
        "\n",
        "        handle.remove()\n",
        "        return activations['input'], activations['output']\n",
        "\n",
        "    def quantize_layer(self,\n",
        "                      layer_name: str,\n",
        "                      layer: Union[nn.Conv2d, nn.Linear],\n",
        "                      calibration_inputs: torch.Tensor,\n",
        "                      original_outputs: torch.Tensor,\n",
        "                      bit_width: int,\n",
        "                      num_iterations: int = 300,\n",
        "                      lr: float = 1e-3) -> Union[QuantizedConv2d, QuantizedLinear]:\n",
        "\n",
        "        logger.info(f\"Quantizing {layer_name} to {bit_width}-bit...\")\n",
        "\n",
        "\n",
        "        if isinstance(layer, nn.Conv2d):\n",
        "            q_layer = QuantizedConv2d(layer, bit_width, self.quantize_activations).to(self.device)\n",
        "        else:\n",
        "            q_layer = QuantizedLinear(layer, bit_width, self.quantize_activations).to(self.device)\n",
        "\n",
        "\n",
        "        params = [q_layer.weight_scale]\n",
        "        if q_layer.bias_float is not None:\n",
        "            params.append(q_layer.bias_scale)\n",
        "        if self.quantize_activations:\n",
        "            if hasattr(q_layer, 'act_scale'):\n",
        "                params.append(q_layer.act_scale)\n",
        "\n",
        "\n",
        "        optimizer = optim.AdamW(params, lr=lr, weight_decay=1e-5)\n",
        "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, num_iterations)\n",
        "\n",
        "        best_loss = float('inf')\n",
        "        best_state = {p: p.data.clone() for p in params}\n",
        "\n",
        "        q_layer.train()\n",
        "        for iteration in range(num_iterations):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            quantized_output = q_layer(calibration_inputs)\n",
        "\n",
        "            loss, metrics = self._compute_rd_loss(\n",
        "                original_outputs, quantized_output, q_layer, use_hessian=(iteration % 10 == 0)\n",
        "            )\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(params, max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            if loss.item() < best_loss:\n",
        "                best_loss = loss.item()\n",
        "                best_state = {p: p.data.clone() for p in params}\n",
        "\n",
        "            if iteration % 50 == 0 or iteration == num_iterations - 1:\n",
        "                logger.info(f\"  Iter {iteration:3d}/{num_iterations} | \"\n",
        "                          f\"Loss: {metrics['total']:.6f} | \"\n",
        "                          f\"D: {metrics['distortion']:.6f} | \"\n",
        "                          f\"R: {metrics['rate']:.4f}\")\n",
        "\n",
        "\n",
        "        for p in params:\n",
        "            p.data = best_state[p]\n",
        "\n",
        "        q_layer.eval()\n",
        "\n",
        "\n",
        "        if hasattr(q_layer, 'compute_compression_rate'):\n",
        "            comp_rate = q_layer.compute_compression_rate()\n",
        "            logger.info(f\"  Effective bits/weight: {comp_rate:.2f}\")\n",
        "\n",
        "        return q_layer\n",
        "\n",
        "    def quantize_block(self,\n",
        "                      block: List[Tuple[str, nn.Module]],\n",
        "                      calibration_data: torch.Tensor,\n",
        "                      num_iterations: int = 300) -> List[Union[QuantizedConv2d, QuantizedLinear]]:\n",
        "\n",
        "        quantized_block = []\n",
        "\n",
        "        for layer_name, layer in block:\n",
        "\n",
        "            bit_width = self.layer_bit_widths.get(layer_name, self.default_bit_width)\n",
        "\n",
        "\n",
        "            layer_inputs, layer_outputs = self._get_layer_activations(layer_name, calibration_data)\n",
        "\n",
        "\n",
        "            q_layer = self.quantize_layer(\n",
        "                layer_name, layer, layer_inputs, layer_outputs,\n",
        "                bit_width, num_iterations\n",
        "            )\n",
        "\n",
        "            quantized_block.append(q_layer)\n",
        "            self.quantized_layers[layer_name] = q_layer\n",
        "\n",
        "        return quantized_block\n",
        "\n",
        "    def quantize_model(self,\n",
        "                      calibration_data: torch.Tensor,\n",
        "                      num_iterations: int = 300,\n",
        "                      analyze_sensitivity: bool = True) -> nn.Module:\n",
        "\n",
        "        calibration_data = calibration_data.to(self.device)\n",
        "\n",
        "\n",
        "        if self.mixed_precision and analyze_sensitivity:\n",
        "            self.analyze_layer_sensitivity(calibration_data)\n",
        "            self.assign_bit_widths()\n",
        "        else:\n",
        "            for name, _ in self._get_quantizable_layers():\n",
        "                self.layer_bit_widths[name] = self.default_bit_width\n",
        "\n",
        "\n",
        "        quantizable_layers = self._get_quantizable_layers()\n",
        "        blocks = self._get_layer_blocks(quantizable_layers)\n",
        "\n",
        "        logger.info(f\"\\nQuantizing {len(quantizable_layers)} layers in {len(blocks)} blocks\")\n",
        "\n",
        "\n",
        "        quantized_model = copy.deepcopy(self.model)\n",
        "\n",
        "\n",
        "        for block_idx, block in enumerate(blocks):\n",
        "            logger.info(f\"\\n[Block {block_idx + 1}/{len(blocks)}]\")\n",
        "\n",
        "            quantized_block = self.quantize_block(block, calibration_data, num_iterations)\n",
        "\n",
        "\n",
        "            for (layer_name, _), q_layer in zip(block, quantized_block):\n",
        "                self._replace_layer(quantized_model, layer_name, q_layer)\n",
        "\n",
        "        logger.info(\"\\nâœ“ Quantization complete!\")\n",
        "        return quantized_model\n",
        "\n",
        "    def _get_layer_by_name(self, model: nn.Module, layer_name: str) -> nn.Module:\n",
        "\n",
        "        parts = layer_name.split('.')\n",
        "        layer = model\n",
        "        for part in parts:\n",
        "            layer = getattr(layer, part)\n",
        "        return layer\n",
        "\n",
        "    def _replace_layer(self, model: nn.Module, layer_name: str, new_layer: nn.Module):\n",
        "\n",
        "        parts = layer_name.split('.')\n",
        "        parent = model\n",
        "        for part in parts[:-1]:\n",
        "            parent = getattr(parent, part)\n",
        "        setattr(parent, parts[-1], new_layer)\n",
        "\n",
        "    def evaluate_model(self,\n",
        "                      original_model: nn.Module,\n",
        "                      quantized_model: nn.Module,\n",
        "                      test_data: torch.Tensor) -> Dict[str, float]:\n",
        "\n",
        "        original_model.eval()\n",
        "        quantized_model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            orig_out = original_model(test_data)\n",
        "            quant_out = quantized_model(test_data)\n",
        "\n",
        "            mse = torch.mean((orig_out - quant_out) ** 2).item()\n",
        "            max_val = orig_out.abs().max().item()\n",
        "            psnr = 10 * np.log10(max_val**2 / (mse + 1e-10)) if mse > 0 else float('inf')\n",
        "\n",
        "            rel_error = (torch.mean(torch.abs(orig_out - quant_out)) /\n",
        "                        (torch.mean(torch.abs(orig_out)) + 1e-10)).item() * 100\n",
        "\n",
        "            cosine_sim = nn.functional.cosine_similarity(\n",
        "                orig_out.flatten(), quant_out.flatten(), dim=0\n",
        "            ).item()\n",
        "\n",
        "\n",
        "        orig_params = sum(p.numel() for p in original_model.parameters())\n",
        "        orig_size_mb = orig_params * 4 / (1024 ** 2)\n",
        "\n",
        "\n",
        "        quant_size_bits = 0\n",
        "        for name, q_layer in self.quantized_layers.items():\n",
        "            bit_width = self.layer_bit_widths.get(name, self.default_bit_width)\n",
        "            if isinstance(q_layer, (QuantizedConv2d, QuantizedLinear)):\n",
        "                num_params = q_layer.weight_float.numel()\n",
        "                if q_layer.bias_float is not None:\n",
        "                    num_params += q_layer.bias_float.numel()\n",
        "                quant_size_bits += num_params * bit_width\n",
        "\n",
        "        quant_size_mb = quant_size_bits / (8 * 1024 ** 2)\n",
        "        compression_ratio = orig_size_mb / quant_size_mb if quant_size_mb > 0 else 0\n",
        "\n",
        "        metrics = {\n",
        "            'mse': mse,\n",
        "            'psnr_db': psnr,\n",
        "            'relative_error_pct': rel_error,\n",
        "            'cosine_similarity': cosine_sim,\n",
        "            'original_size_mb': orig_size_mb,\n",
        "            'quantized_size_mb': quant_size_mb,\n",
        "            'compression_ratio': compression_ratio\n",
        "        }\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def export_quantized_model(self,\n",
        "                               quantized_model: nn.Module,\n",
        "                               output_path: str = 'quantized_model.pth'):\n",
        "\n",
        "        export_dict = {\n",
        "            'model_state': quantized_model.state_dict(),\n",
        "            'quantized_params': {},\n",
        "            'bit_widths': self.layer_bit_widths,\n",
        "            'config': {\n",
        "                'default_bit_width': self.default_bit_width,\n",
        "                'lambda_rd': self.lambda_rd,\n",
        "                'quantize_activations': self.quantize_activations\n",
        "            }\n",
        "        }\n",
        "\n",
        "        for name, q_layer in self.quantized_layers.items():\n",
        "            export_dict['quantized_params'][name] = q_layer.get_quantized_params()\n",
        "\n",
        "        torch.save(export_dict, output_path)\n",
        "        logger.info(f\"Exported quantized model to {output_path}\")\n",
        "\n",
        "        return export_dict\n",
        "\n",
        "\n",
        "def demo_production_rdo_ptq():\n",
        "\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(\"Production RDO-PTQ: Full-Featured Quantization Pipeline\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "\n",
        "    class ResNetBlock(nn.Module):\n",
        "        def __init__(self, in_channels, out_channels, stride=1):\n",
        "            super().__init__()\n",
        "            self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride=stride, padding=1)\n",
        "            self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "            self.relu = nn.ReLU(inplace=True)\n",
        "            self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
        "            self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "            self.downsample = None\n",
        "            if stride != 1 or in_channels != out_channels:\n",
        "                self.downsample = nn.Sequential(\n",
        "                    nn.Conv2d(in_channels, out_channels, 1, stride=stride),\n",
        "                    nn.BatchNorm2d(out_channels)\n",
        "                )\n",
        "\n",
        "        def forward(self, x):\n",
        "            identity = x\n",
        "\n",
        "            out = self.conv1(x)\n",
        "            out = self.bn1(out)\n",
        "            out = self.relu(out)\n",
        "\n",
        "            out = self.conv2(out)\n",
        "            out = self.bn2(out)\n",
        "\n",
        "            if self.downsample is not None:\n",
        "                identity = self.downsample(x)\n",
        "\n",
        "            out += identity\n",
        "            out = self.relu(out)\n",
        "            return out\n",
        "\n",
        "    class ImageClassifier(nn.Module):\n",
        "        def __init__(self, num_classes=10):\n",
        "            super().__init__()\n",
        "            self.conv1 = nn.Conv2d(3, 64, 7, stride=2, padding=3)\n",
        "            self.bn1 = nn.BatchNorm2d(64)\n",
        "            self.relu = nn.ReLU(inplace=True)\n",
        "            self.maxpool = nn.MaxPool2d(3, stride=2, padding=1)\n",
        "\n",
        "            self.layer1 = ResNetBlock(64, 64)\n",
        "            self.layer2 = ResNetBlock(64, 128, stride=2)\n",
        "            self.layer3 = ResNetBlock(128, 256, stride=2)\n",
        "\n",
        "            self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "            self.fc = nn.Linear(256, num_classes)\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = self.conv1(x)\n",
        "            x = self.bn1(x)\n",
        "            x = self.relu(x)\n",
        "            x = self.maxpool(x)\n",
        "\n",
        "            x = self.layer1(x)\n",
        "            x = self.layer2(x)\n",
        "            x = self.layer3(x)\n",
        "\n",
        "            x = self.avgpool(x)\n",
        "            x = torch.flatten(x, 1)\n",
        "            x = self.fc(x)\n",
        "            return x\n",
        "\n",
        "    print(\"\\n[Step 1] Creating pre-trained model...\")\n",
        "    model = ImageClassifier(num_classes=10)\n",
        "\n",
        "\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.Linear):\n",
        "            nn.init.normal_(m.weight, 0, 0.01)\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    fp32_size = total_params * 4 / (1024 ** 2)\n",
        "    print(f\"Total parameters: {total_params:,}\")\n",
        "    print(f\"FP32 model size: {fp32_size:.2f} MB\")\n",
        "\n",
        "\n",
        "    print(\"\\n[Step 2] Preparing calibration data...\")\n",
        "    num_calibration = 50\n",
        "    calibration_data = torch.randn(num_calibration, 3, 224, 224)\n",
        "    print(f\"Calibration set: {num_calibration} images (224x224)\")\n",
        "\n",
        "\n",
        "    print(\"\\n[Step 3] Initializing Production RDO-PTQ...\")\n",
        "    print(\"Features enabled:\")\n",
        "    print(\"  âœ“ Activation quantization\")\n",
        "    print(\"  âœ“ Block-wise reconstruction\")\n",
        "    print(\"  âœ“ Mixed precision search\")\n",
        "    print(\"  âœ“ Hessian-guided optimization\")\n",
        "\n",
        "    quantizer = ProductionRDO_PTQ(\n",
        "        model=model,\n",
        "        bit_width=8,\n",
        "        lambda_rd=0.005,\n",
        "        quantize_activations=True,\n",
        "        use_blocks=True,\n",
        "        mixed_precision=True,\n",
        "        device='cpu'\n",
        "    )\n",
        "\n",
        "\n",
        "    print(\"\\n[Step 4] Running quantization pipeline...\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    quantized_model = quantizer.quantize_model(\n",
        "        calibration_data=calibration_data,\n",
        "        num_iterations=150,\n",
        "        analyze_sensitivity=True\n",
        "    )\n",
        "\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    print(\"\\n[Step 5] Evaluating quantized model...\")\n",
        "    test_data = torch.randn(20, 3, 224, 224)\n",
        "\n",
        "    metrics = quantizer.evaluate_model(model, quantized_model, test_data)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"QUANTIZATION RESULTS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(\"\\n Compression Metrics:\")\n",
        "    print(f\"  Original size:    {metrics['original_size_mb']:.2f} MB\")\n",
        "    print(f\"  Quantized size:   {metrics['quantized_size_mb']:.2f} MB\")\n",
        "    print(f\"  Compression:      {metrics['compression_ratio']:.2f}x\")\n",
        "\n",
        "    print(\"\\n Accuracy Metrics:\")\n",
        "    print(f\"  MSE:              {metrics['mse']:.8f}\")\n",
        "    print(f\"  PSNR:             {metrics['psnr_db']:.2f} dB\")\n",
        "    print(f\"  Relative Error:   {metrics['relative_error_pct']:.2f}%\")\n",
        "    print(f\"  Cosine Sim:       {metrics['cosine_similarity']:.4f}\")\n",
        "\n",
        "    print(\"\\n Per-Layer Bit-Widths:\")\n",
        "    for layer_name, bit_width in list(quantizer.layer_bit_widths.items())[:10]:\n",
        "        print(f\"  {layer_name:30s} â†’ {bit_width}-bit\")\n",
        "    if len(quantizer.layer_bit_widths) > 10:\n",
        "        print(f\"  ... and {len(quantizer.layer_bit_widths) - 10} more layers\")\n",
        "\n",
        "    print(\"\\n[Step 6] Exporting quantized model...\")\n",
        "    export_dict = quantizer.export_quantized_model(quantized_model, 'production_quantized.pth')\n",
        "\n",
        "    print(f\"\\n Export complete:\")\n",
        "    print(f\"  Model state:      {len(export_dict['model_state'])} tensors\")\n",
        "    print(f\"  Quantized params: {len(export_dict['quantized_params'])} layers\")\n",
        "    print(f\"  File saved:       production_quantized.pth\")\n",
        "\n",
        "\n",
        "    print(\"\\n[Step 7] Deployment Info:\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    return model, quantized_model, quantizer\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    original_model, quantized_model, quantizer = demo_production_rdo_ptq()\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQHuOLcpmDYM",
        "outputId": "c9d94cb6-9bf8-4514-cee1-96b45c8c8e3a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Production RDO-PTQ: Full-Featured Quantization Pipeline\n",
            "================================================================================\n",
            "\n",
            "[Step 1] Creating pre-trained model...\n",
            "Total parameters: 1,236,618\n",
            "FP32 model size: 4.72 MB\n",
            "\n",
            "[Step 2] Preparing calibration data...\n",
            "Calibration set: 50 images (224x224)\n",
            "\n",
            "[Step 3] Initializing Production RDO-PTQ...\n",
            "Features enabled:\n",
            "  âœ“ Activation quantization\n",
            "  âœ“ Block-wise reconstruction\n",
            "  âœ“ Mixed precision search\n",
            "  âœ“ Hessian-guided optimization\n",
            "\n",
            "[Step 4] Running quantization pipeline...\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "[Step 5] Evaluating quantized model...\n",
            "\n",
            "================================================================================\n",
            "QUANTIZATION RESULTS\n",
            "================================================================================\n",
            "\n",
            " Compression Metrics:\n",
            "  Original size:    4.72 MB\n",
            "  Quantized size:   0.68 MB\n",
            "  Compression:      6.96x\n",
            "\n",
            " Accuracy Metrics:\n",
            "  MSE:              0.00197210\n",
            "  PSNR:             9.82 dB\n",
            "  Relative Error:   62.41%\n",
            "  Cosine Sim:       0.7467\n",
            "\n",
            " Per-Layer Bit-Widths:\n",
            "  conv1                          â†’ 4-bit\n",
            "  layer1.conv1                   â†’ 4-bit\n",
            "  layer1.conv2                   â†’ 4-bit\n",
            "  layer2.conv1                   â†’ 6-bit\n",
            "  layer2.conv2                   â†’ 4-bit\n",
            "  layer2.downsample.0            â†’ 4-bit\n",
            "  layer3.conv1                   â†’ 6-bit\n",
            "  layer3.conv2                   â†’ 4-bit\n",
            "  layer3.downsample.0            â†’ 4-bit\n",
            "  fc                             â†’ 8-bit\n",
            "\n",
            "[Step 6] Exporting quantized model...\n",
            "\n",
            " Export complete:\n",
            "  Model state:      124 tensors\n",
            "  Quantized params: 10 layers\n",
            "  File saved:       production_quantized.pth\n",
            "\n",
            "[Step 7] Deployment Info:\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#FixedRDOPTQ\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from typing import List, Tuple, Dict\n",
        "import copy\n",
        "import numpy as np\n",
        "\n",
        "print(\"Importing RDO-PTQ modules...\")\n",
        "\n",
        "class QuantizedConv2d(nn.Module):\n",
        "\n",
        "    def __init__(self, conv_layer: nn.Conv2d, bit_width: int = 8):\n",
        "        super().__init__()\n",
        "        self.in_channels = conv_layer.in_channels\n",
        "        self.out_channels = conv_layer.out_channels\n",
        "        self.kernel_size = conv_layer.kernel_size\n",
        "        self.stride = conv_layer.stride\n",
        "        self.padding = conv_layer.padding\n",
        "        self.bit_width = bit_width\n",
        "\n",
        "        self.qmax = (2 ** (bit_width - 1)) - 1\n",
        "        self.qmin = -(2 ** (bit_width - 1))\n",
        "\n",
        "        self.register_buffer('weight_float', conv_layer.weight.data.clone())\n",
        "        if conv_layer.bias is not None:\n",
        "            self.register_buffer('bias_float', conv_layer.bias.data.clone())\n",
        "        else:\n",
        "            self.bias_float = None\n",
        "\n",
        "        self.weight_scale = nn.Parameter(torch.ones(self.out_channels, 1, 1, 1))\n",
        "\n",
        "        if self.bias_float is not None:\n",
        "            self.bias_scale = nn.Parameter(torch.ones(self.out_channels))\n",
        "\n",
        "        self._initialize_scales()\n",
        "\n",
        "    def _initialize_scales(self):\n",
        "        with torch.no_grad():\n",
        "            for c in range(self.out_channels):\n",
        "                w_channel = self.weight_float[c]\n",
        "                w_max_abs = torch.max(torch.abs(w_channel))\n",
        "                self.weight_scale.data[c] = (w_max_abs / (self.qmax + 1e-8)).clamp(min=1e-8)\n",
        "\n",
        "            if self.bias_float is not None:\n",
        "                for c in range(self.out_channels):\n",
        "                    b_abs = torch.abs(self.bias_float[c])\n",
        "                    self.bias_scale.data[c] = (b_abs / (self.qmax + 1e-8)).clamp(min=1e-8)\n",
        "\n",
        "    def quantize_weights(self):\n",
        "        w_div = self.weight_float / self.weight_scale.clamp(min=1e-8)\n",
        "        w_quant = torch.clamp(torch.round(w_div), self.qmin, self.qmax)\n",
        "        return w_quant * self.weight_scale\n",
        "\n",
        "    def quantize_bias(self):\n",
        "        if self.bias_float is None:\n",
        "            return None\n",
        "        b_div = self.bias_float / self.bias_scale.clamp(min=1e-8)\n",
        "        b_quant = torch.clamp(torch.round(b_div), self.qmin, self.qmax)\n",
        "        return b_quant * self.bias_scale\n",
        "\n",
        "    def forward(self, x):\n",
        "        w_quant = self.quantize_weights()\n",
        "        b_quant = self.quantize_bias()\n",
        "        return nn.functional.conv2d(x, w_quant, b_quant,\n",
        "                                   stride=self.stride, padding=self.padding)\n",
        "\n",
        "    def get_quantized_params(self):\n",
        "        with torch.no_grad():\n",
        "            w_div = self.weight_float / self.weight_scale.clamp(min=1e-8)\n",
        "            w_int8 = torch.clamp(torch.round(w_div), self.qmin, self.qmax).to(torch.int8)\n",
        "\n",
        "            result = {'weight_int8': w_int8, 'weight_scale': self.weight_scale.clone()}\n",
        "\n",
        "            if self.bias_float is not None:\n",
        "                b_div = self.bias_float / self.bias_scale.clamp(min=1e-8)\n",
        "                b_int8 = torch.clamp(torch.round(b_div), self.qmin, self.qmax).to(torch.int8)\n",
        "                result['bias_int8'] = b_int8\n",
        "                result['bias_scale'] = self.bias_scale.clone()\n",
        "\n",
        "        return result\n",
        "\n",
        "\n",
        "class FixedRDO_PTQ:\n",
        "\n",
        "\n",
        "    def __init__(self,\n",
        "                 model: nn.Module,\n",
        "                 bit_width: int = 8,\n",
        "                 lambda_rd: float = 0.01,\n",
        "                 mixed_precision: bool = False,\n",
        "                 device: str = 'cpu'):\n",
        "        self.model = model.to(device)\n",
        "        self.default_bit_width = bit_width\n",
        "        self.lambda_rd = lambda_rd\n",
        "        self.mixed_precision = mixed_precision\n",
        "        self.device = device\n",
        "\n",
        "        self.quantized_layers = {}\n",
        "        self.layer_bit_widths = {}\n",
        "        self.layer_sensitivities = {}\n",
        "\n",
        "    def _get_quantizable_layers(self) -> List[Tuple[str, nn.Module]]:\n",
        "        quantizable = []\n",
        "        for name, module in self.model.named_modules():\n",
        "            if isinstance(module, nn.Conv2d):\n",
        "                quantizable.append((name, module))\n",
        "        print(f\"DEBUG: Found {len(quantizable)} Conv2d layers\")\n",
        "        return quantizable\n",
        "\n",
        "    def analyze_sensitivity(self, calibration_data: torch.Tensor) -> Dict[str, float]:\n",
        "\n",
        "        print(\"\\n Analyzing layer sensitivities...\")\n",
        "\n",
        "        quantizable_layers = self._get_quantizable_layers()\n",
        "        sensitivities = {}\n",
        "\n",
        "\n",
        "        data_subset = calibration_data[:min(10, len(calibration_data))].to(self.device)\n",
        "\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            baseline = self.model(data_subset)\n",
        "\n",
        "        for idx, (layer_name, layer) in enumerate(quantizable_layers):\n",
        "            print(f\"  [{idx+1}/{len(quantizable_layers)}] Testing {layer_name}...\", end=' ')\n",
        "\n",
        "\n",
        "            temp_q = QuantizedConv2d(layer, bit_width=4).to(self.device)\n",
        "            temp_q.eval()\n",
        "\n",
        "\n",
        "            parts = layer_name.split('.')\n",
        "            parent = self.model\n",
        "            for part in parts[:-1]:\n",
        "                parent = getattr(parent, part)\n",
        "            original = getattr(parent, parts[-1])\n",
        "\n",
        "\n",
        "            setattr(parent, parts[-1], temp_q)\n",
        "\n",
        "\n",
        "            with torch.no_grad():\n",
        "                quantized = self.model(data_subset)\n",
        "                sens = torch.mean((baseline - quantized) ** 2).item()\n",
        "\n",
        "            sensitivities[layer_name] = sens\n",
        "            print(f\"sensitivity={sens:.6f}\")\n",
        "\n",
        "\n",
        "            setattr(parent, parts[-1], original)\n",
        "\n",
        "        self.layer_sensitivities = sensitivities\n",
        "        return sensitivities\n",
        "\n",
        "    def assign_bit_widths(self):\n",
        "\n",
        "        if not self.layer_sensitivities:\n",
        "            for name, _ in self._get_quantizable_layers():\n",
        "                self.layer_bit_widths[name] = self.default_bit_width\n",
        "            return\n",
        "\n",
        "        print(\"\\n Assigning bit-widths...\")\n",
        "\n",
        "        sens_list = np.array(list(self.layer_sensitivities.values()))\n",
        "        if len(sens_list) == 0:\n",
        "            return\n",
        "\n",
        "        sens_min, sens_max = sens_list.min(), sens_list.max()\n",
        "\n",
        "        for name, sens in self.layer_sensitivities.items():\n",
        "\n",
        "            if sens_max > sens_min:\n",
        "                norm_sens = (sens - sens_min) / (sens_max - sens_min)\n",
        "            else:\n",
        "                norm_sens = 0.5\n",
        "\n",
        "\n",
        "            if norm_sens > 0.7:\n",
        "                bits = 8\n",
        "            elif norm_sens > 0.3:\n",
        "                bits = 6\n",
        "            else:\n",
        "                bits = 4\n",
        "\n",
        "            self.layer_bit_widths[name] = bits\n",
        "            print(f\"  {name:30s} â†’ {bits}-bit (sens={norm_sens:.3f})\")\n",
        "\n",
        "    def _get_activations(self, layer_name: str, inputs: torch.Tensor):\n",
        "\n",
        "        acts = {}\n",
        "\n",
        "        def hook(module, inp, out):\n",
        "            acts['input'] = inp[0].detach().clone()\n",
        "            acts['output'] = out.detach().clone()\n",
        "\n",
        "\n",
        "        parts = layer_name.split('.')\n",
        "        layer = self.model\n",
        "        for part in parts:\n",
        "            layer = getattr(layer, part)\n",
        "\n",
        "        handle = layer.register_forward_hook(hook)\n",
        "\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            _ = self.model(inputs)\n",
        "\n",
        "        handle.remove()\n",
        "        return acts['input'], acts['output']\n",
        "\n",
        "    def quantize_layer(self,\n",
        "                      layer_name: str,\n",
        "                      layer: nn.Conv2d,\n",
        "                      layer_inputs: torch.Tensor,\n",
        "                      layer_outputs: torch.Tensor,\n",
        "                      bit_width: int,\n",
        "                      num_iters: int = 100) -> QuantizedConv2d:\n",
        "\n",
        "\n",
        "        print(f\"  Quantizing {layer_name} â†’ {bit_width}-bit\")\n",
        "\n",
        "        q_layer = QuantizedConv2d(layer, bit_width).to(self.device)\n",
        "\n",
        "\n",
        "        params = [q_layer.weight_scale]\n",
        "        if q_layer.bias_float is not None:\n",
        "            params.append(q_layer.bias_scale)\n",
        "\n",
        "        optimizer = optim.Adam(params, lr=1e-3)\n",
        "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, num_iters)\n",
        "\n",
        "        best_loss = float('inf')\n",
        "        best_state = [p.data.clone() for p in params]\n",
        "\n",
        "        q_layer.train()\n",
        "        for it in range(num_iters):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            q_out = q_layer(layer_inputs)\n",
        "\n",
        "\n",
        "            distortion = torch.mean((layer_outputs - q_out) ** 2)\n",
        "            rate = torch.mean(torch.log(q_layer.weight_scale.clamp(min=1e-8) + 1e-8))\n",
        "            loss = distortion + self.lambda_rd * rate\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(params, 1.0)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            if loss.item() < best_loss:\n",
        "                best_loss = loss.item()\n",
        "                best_state = [p.data.clone() for p in params]\n",
        "\n",
        "            if it % 25 == 0 or it == num_iters - 1:\n",
        "                print(f\"    Iter {it:3d}/{num_iters} | Loss: {loss.item():.6f} | \"\n",
        "                      f\"D: {distortion.item():.6f} | R: {rate.item():.4f}\")\n",
        "\n",
        "\n",
        "        for p, best in zip(params, best_state):\n",
        "            p.data = best\n",
        "\n",
        "        q_layer.eval()\n",
        "        return q_layer\n",
        "\n",
        "    def quantize_model(self,\n",
        "                      calibration_data: torch.Tensor,\n",
        "                      num_iterations: int = 100) -> nn.Module:\n",
        "\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"STARTING QUANTIZATION\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        calibration_data = calibration_data.to(self.device)\n",
        "\n",
        "\n",
        "        quantizable_layers = self._get_quantizable_layers()\n",
        "\n",
        "        if len(quantizable_layers) == 0:\n",
        "            print(\"ERROR: No quantizable layers found!\")\n",
        "            return self.model\n",
        "\n",
        "\n",
        "        if self.mixed_precision:\n",
        "            self.analyze_sensitivity(calibration_data)\n",
        "            self.assign_bit_widths()\n",
        "        else:\n",
        "            print(f\"\\n Using uniform {self.default_bit_width}-bit quantization\")\n",
        "            for name, _ in quantizable_layers:\n",
        "                self.layer_bit_widths[name] = self.default_bit_width\n",
        "\n",
        "\n",
        "        print(f\"\\n Quantizing {len(quantizable_layers)} layers...\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        quantized_model = copy.deepcopy(self.model)\n",
        "\n",
        "        for idx, (layer_name, layer) in enumerate(quantizable_layers):\n",
        "            print(f\"\\n[Layer {idx+1}/{len(quantizable_layers)}] {layer_name}\")\n",
        "\n",
        "\n",
        "            layer_in, layer_out = self._get_activations(layer_name, calibration_data)\n",
        "\n",
        "\n",
        "            bit_width = self.layer_bit_widths.get(layer_name, self.default_bit_width)\n",
        "\n",
        "\n",
        "            q_layer = self.quantize_layer(\n",
        "                layer_name, layer, layer_in, layer_out,\n",
        "                bit_width, num_iterations\n",
        "            )\n",
        "\n",
        "\n",
        "            parts = layer_name.split('.')\n",
        "            parent = quantized_model\n",
        "            for part in parts[:-1]:\n",
        "                parent = getattr(parent, part)\n",
        "            setattr(parent, parts[-1], q_layer)\n",
        "\n",
        "            self.quantized_layers[layer_name] = q_layer\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"âœ“ QUANTIZATION COMPLETE\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        return quantized_model\n",
        "\n",
        "    def evaluate(self, orig_model, quant_model, test_data):\n",
        "\n",
        "        orig_model.eval()\n",
        "        quant_model.eval()\n",
        "\n",
        "        test_data = test_data.to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            orig_out = orig_model(test_data)\n",
        "            quant_out = quant_model(test_data)\n",
        "\n",
        "            mse = torch.mean((orig_out - quant_out) ** 2).item()\n",
        "            max_val = orig_out.abs().max().item()\n",
        "            psnr = 10 * np.log10(max_val**2 / (mse + 1e-10)) if mse > 0 else 100.0\n",
        "\n",
        "            rel_err = (torch.mean(torch.abs(orig_out - quant_out)) /\n",
        "                      (torch.mean(torch.abs(orig_out)) + 1e-10)).item() * 100\n",
        "\n",
        "            cos_sim = nn.functional.cosine_similarity(\n",
        "                orig_out.flatten(), quant_out.flatten(), dim=0\n",
        "            ).item()\n",
        "\n",
        "\n",
        "        orig_params = sum(p.numel() for p in orig_model.parameters())\n",
        "        orig_size_mb = orig_params * 4 / (1024 ** 2)\n",
        "\n",
        "        quant_bits = 0\n",
        "        for name, q_layer in self.quantized_layers.items():\n",
        "            bits = self.layer_bit_widths.get(name, 8)\n",
        "            quant_bits += q_layer.weight_float.numel() * bits\n",
        "            if q_layer.bias_float is not None:\n",
        "                quant_bits += q_layer.bias_float.numel() * bits\n",
        "\n",
        "        quant_size_mb = quant_bits / (8 * 1024 ** 2)\n",
        "        compression = orig_size_mb / quant_size_mb if quant_size_mb > 0 else 0\n",
        "\n",
        "        return {\n",
        "            'mse': mse,\n",
        "            'psnr_db': psnr,\n",
        "            'relative_error_pct': rel_err,\n",
        "            'cosine_similarity': cos_sim,\n",
        "            'original_size_mb': orig_size_mb,\n",
        "            'quantized_size_mb': quant_size_mb,\n",
        "            'compression_ratio': compression\n",
        "        }\n",
        "\n",
        "\n",
        "def demo():\n",
        "\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(\"Fixed Production RDO-PTQ Demo\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "\n",
        "    class SimpleModel(nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "            self.features = nn.Sequential(\n",
        "                nn.Conv2d(3, 32, 3, padding=1),\n",
        "                nn.ReLU(),\n",
        "                nn.Conv2d(32, 64, 3, stride=2, padding=1),\n",
        "                nn.ReLU(),\n",
        "                nn.Conv2d(64, 128, 3, stride=2, padding=1),\n",
        "                nn.ReLU(),\n",
        "            )\n",
        "            self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "            self.fc = nn.Linear(128, 10)\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = self.features(x)\n",
        "            x = self.avgpool(x)\n",
        "            x = torch.flatten(x, 1)\n",
        "            x = self.fc(x)\n",
        "            return x\n",
        "\n",
        "    print(\"\\n[1] Creating model...\")\n",
        "    model = SimpleModel()\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            nn.init.kaiming_normal_(m.weight)\n",
        "            if m.bias is not None:\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"Total parameters: {total_params:,}\")\n",
        "    print(f\"FP32 size: {total_params * 4 / 1024**2:.2f} MB\")\n",
        "\n",
        "    print(\"\\n[2] Preparing calibration data...\")\n",
        "    calib_data = torch.randn(30, 3, 64, 64)\n",
        "    print(f\"Calibration: {len(calib_data)} images\")\n",
        "\n",
        "    print(\"\\n[3] Initializing quantizer...\")\n",
        "    quantizer = FixedRDO_PTQ(\n",
        "        model=model,\n",
        "        bit_width=8,\n",
        "        lambda_rd=0.01,\n",
        "        mixed_precision=True,\n",
        "        device='cpu'\n",
        "    )\n",
        "\n",
        "    print(\"\\n[4] Running quantization...\")\n",
        "    quant_model = quantizer.quantize_model(calib_data, num_iterations=50)\n",
        "\n",
        "    print(\"\\n[5] Evaluation...\")\n",
        "    test_data = torch.randn(20, 3, 64, 64)\n",
        "    metrics = quantizer.evaluate(model, quant_model, test_data)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"RESULTS\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"\\n Compression:\")\n",
        "    print(f\"  Original:    {metrics['original_size_mb']:.2f} MB\")\n",
        "    print(f\"  Quantized:   {metrics['quantized_size_mb']:.2f} MB\")\n",
        "    print(f\"  Ratio:       {metrics['compression_ratio']:.2f}x\")\n",
        "\n",
        "    print(f\"\\n Accuracy:\")\n",
        "    print(f\"  PSNR:        {metrics['psnr_db']:.2f} dB\")\n",
        "    print(f\"  Rel Error:   {metrics['relative_error_pct']:.2f}%\")\n",
        "    print(f\"  Cosine Sim:  {metrics['cosine_similarity']:.4f}\")\n",
        "\n",
        "    print(f\"\\n Bit-widths:\")\n",
        "    for name, bits in list(quantizer.layer_bit_widths.items())[:5]:\n",
        "        print(f\"  {name:25s} â†’ {bits}-bit\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"âœ“ Demo complete!\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    return model, quant_model\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVBTXb49xmBW",
        "outputId": "85232977-06e6-40a6-b2ef-af852086014a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Importing RDO-PTQ modules...\n",
            "================================================================================\n",
            "Fixed Production RDO-PTQ Demo\n",
            "================================================================================\n",
            "\n",
            "[1] Creating model...\n",
            "Total parameters: 94,538\n",
            "FP32 size: 0.36 MB\n",
            "\n",
            "[2] Preparing calibration data...\n",
            "Calibration: 30 images\n",
            "\n",
            "[3] Initializing quantizer...\n",
            "\n",
            "[4] Running quantization...\n",
            "\n",
            "================================================================================\n",
            "STARTING QUANTIZATION\n",
            "================================================================================\n",
            "DEBUG: Found 3 Conv2d layers\n",
            "\n",
            " Analyzing layer sensitivities...\n",
            "DEBUG: Found 3 Conv2d layers\n",
            "  [1/3] Testing features.0... sensitivity=0.000031\n",
            "  [2/3] Testing features.2... sensitivity=0.000643\n",
            "  [3/3] Testing features.4... sensitivity=0.004673\n",
            "\n",
            " Assigning bit-widths...\n",
            "  features.0                     â†’ 4-bit (sens=0.000)\n",
            "  features.2                     â†’ 4-bit (sens=0.132)\n",
            "  features.4                     â†’ 8-bit (sens=1.000)\n",
            "\n",
            " Quantizing 3 layers...\n",
            "================================================================================\n",
            "\n",
            "[Layer 1/3] features.0\n",
            "  Quantizing features.0 â†’ 4-bit\n",
            "    Iter   0/50 | Loss: -0.006547 | D: 0.017696 | R: -2.4244\n",
            "    Iter  25/50 | Loss: -0.008407 | D: 0.016087 | R: -2.4494\n",
            "    Iter  49/50 | Loss: -0.008488 | D: 0.016006 | R: -2.4494\n",
            "\n",
            "[Layer 2/3] features.2\n",
            "  Quantizing features.2 â†’ 4-bit\n",
            "    Iter   0/50 | Loss: -0.003263 | D: 0.029916 | R: -3.3179\n",
            "    Iter  25/50 | Loss: 0.003368 | D: 0.038058 | R: -3.4690\n",
            "    Iter  49/50 | Loss: 0.009198 | D: 0.044016 | R: -3.4817\n",
            "\n",
            "[Layer 3/3] features.4\n",
            "  Quantizing features.4 â†’ 8-bit\n",
            "    Iter   0/50 | Loss: -0.064825 | D: 0.000145 | R: -6.4970\n",
            "    Iter  25/50 | Loss: -0.048952 | D: 0.009756 | R: -5.8708\n",
            "    Iter  49/50 | Loss: -0.051116 | D: 0.009104 | R: -6.0220\n",
            "\n",
            "================================================================================\n",
            "âœ“ QUANTIZATION COMPLETE\n",
            "================================================================================\n",
            "\n",
            "[5] Evaluation...\n",
            "\n",
            "================================================================================\n",
            "RESULTS\n",
            "================================================================================\n",
            "\n",
            " Compression:\n",
            "  Original:    0.36 MB\n",
            "  Quantized:   0.08 MB\n",
            "  Ratio:       4.53x\n",
            "\n",
            " Accuracy:\n",
            "  PSNR:        16.18 dB\n",
            "  Rel Error:   31.94%\n",
            "  Cosine Sim:  0.9687\n",
            "\n",
            " Bit-widths:\n",
            "  features.0                â†’ 4-bit\n",
            "  features.2                â†’ 4-bit\n",
            "  features.4                â†’ 8-bit\n",
            "\n",
            "================================================================================\n",
            "âœ“ Demo complete!\n",
            "================================================================================\n"
          ]
        }
      ]
    }
  ]
}